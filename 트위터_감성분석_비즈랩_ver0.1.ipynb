{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89aa4652",
   "metadata": {},
   "outputs": [],
   "source": [
    "#모듈 임포트\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import os \n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a0b271a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#필요 모델 다운로드 및 파이프라인 생성\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "\n",
    "twitter_nlp = pipeline('sentiment-analysis', model = model, tokenizer = tokenizer, top_k = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22ccbc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#필요 함수 생성\n",
    "#트위터 텍스트 전처리 함수\n",
    "def preprocess(text):\n",
    "    text = str(text).replace('\\n', ' ')\n",
    "    new_text = []\n",
    "    for t in str(text).split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "#예외처리를 위한 트위터 감성 분석 함수\n",
    "def sentiment_analysis(text):\n",
    "    try:\n",
    "        return twitter_nlp(text)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass\n",
    "    \n",
    "#감성 분석 결과 반환 함수\n",
    "def labelling(full_result):\n",
    "    positive_score = 0\n",
    "    neutral_score = 0\n",
    "    negative_score = 0\n",
    "    for i in range(3):\n",
    "        if full_result[0][i]['label'] == 'positive': positive_score = full_result[0][i]['score']\n",
    "        elif full_result[0][i]['label'] == 'neutral': neutral_score = full_result[0][i]['score']\n",
    "        else: negative_score = full_result[0][i]['score']\n",
    "    highest_score = max(positive_score, neutral_score, negative_score)\n",
    "    \n",
    "    for i in range(3):\n",
    "        if full_result[0][i]['score'] == highest_score: max_label = full_result[0][i]['label']\n",
    "    return [positive_score, neutral_score, negative_score, max_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8be16149",
   "metadata": {},
   "outputs": [],
   "source": [
    "#파일 경로 설정\n",
    "#자기 경우에 맞게 파일 읽어들일 디렉토리랑 저장할 디렉토리 설정하기\n",
    "file_path = '/Users/parkjunhyeong/Desktop/박준형/02. 대내 및 대외활동/01. 대내활동/03. Biz&AI 랩/02. 소스/01. 데이터/03. 트위터 데이터/02. Selected Data'\n",
    "save_path = '/Users/parkjunhyeong/Desktop/박준형/02. 대내 및 대외활동/01. 대내활동/03. Biz&AI 랩/02. 소스/01. 데이터/03. 트위터 데이터/02. Selected Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6efe6d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#기업 리스트 설정\n",
    "#하나당 시간이 꽤 걸리므로 한번 돌릴때 한개에서 두개 정도 돌리는 거 추천 (하나에 6시간 ~ 7시간 걸림)\n",
    "#아래 예시\n",
    "short_list = ['JBLU.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b3f81a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#50만개 안 넘는 거 감성 분석하기\n",
    "for s in short_list:\n",
    "    df = pd.read_csv(file_path + '/' + s,index_col = 0, lineterminator = '\\n')\n",
    "    df['clean_content'] = df['content'].apply(lambda x : preprocess(x))\n",
    "    df['sentiment'] = df['clean_content'].progress_apply(lambda x : sentiment_analysis(x))\n",
    "    df.dropna(inplace = True)\n",
    "    df.reset_index(drop = True, inplace = True)\n",
    "    #negative, neutral, positive 칼럼 생성하기\n",
    "    for i in range(len(df)):\n",
    "        labelling_results = labelling(df.loc[i, 'sentiment'])\n",
    "        df.loc[i,'positive'] = labelling_results[0]\n",
    "        df.loc[i,'neutral'] = labelling_results[1]\n",
    "        df.loc[i,'negative'] = labelling_results[2]\n",
    "        df.loc[i,'sentiment'] = labelling_results[3]\n",
    "    #sentiment score 칼럼 생성하기\n",
    "    df['sentiment_score'] = df['positive'] - df['negative']\n",
    "    df = df[['datetime', 'username', 'content', 'clean_content', 'sentiment', 'sentiment_score', 'positive', 'neutral', 'negative']]\n",
    "    df.to_csv(save_path + '/' + s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
